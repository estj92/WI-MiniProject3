\section{Learning}
%We have talked about two basic paradigms of recommender system. Which one fits the above data? What would we need to build the other type? 
The two basic paradigms in recommender systems are Content-based, and collaborative. And hybrid versions. The short version of each is:
\paragraph{Content based} Selects ``things'' similar what to the user previously liked.
\paragraph{Collaborative} Select ``things'' the community likes
\paragraph{Hybrid} Combines different input sources.

Content based, and collaborative filtering will be further described, and the one best fitting this project will be decided.

\subsection{Content based filtering}
This method needs two things; information about items, and a user profile.
The item information could be e.g.\ author, genre, \ldots.
The user profile describes what the user likes, or dislikes, from either implicit or explicit information.
This makes it an adaptive process, as the user might rate new items, or buy new things, or \ldots.

The content can be anything, from websites, to restaurants. Structured (parts of) content can be represented by a set of attributes, while unstructured content can just be a free-text description.

The basic method for suggest items to the user $U$, from a set of item $I$, is to compute the similarity with an item the user has not seen, combining multiple attributes:
\[
    \mathrm{sim}(U, I) = w_1 \mathrm{sim}_{att_1}(U, I) + w_2 \mathrm{sim}_{att_2}(U, I) + \ldots
\]
and suggest the most similar items.
Different attributes are combined, as they might have different similarity measures, or one might be more important than another.
Comparing unstructured text is as simple as using cosine similarity, as we did in the first miniproject.

\subsection{Collaborative filtering}
The are two approaches to CF: User-based nearest neighbors, and item-based nearest neighbors ($k$NN).
Both require a matrix of user-item ratings.

\subsubsection{User-based $k$NN}
It has a limiting assumption: Users preferences and tastes are stable over time.
The general approach is to select an item not seen by a user, and estimate the users rating based on like-minded other users.

To measure user similarity, Pearson Similarity can be used:
\[
    \mathrm{sim}(a, b) = \frac{\sum_{p \in P}(r_{a, p} - \bar{r_a})(r_{b, p} - \bar{r_b})}{\sqrt{\sum_{p \in P}(r_{a, p} - \bar{r_a})^2}\sqrt{\sum_{p \in P}(r_{b, p} - \bar{r_b})^2}}
\]
where
\begin{itemize}
    \item $a, b$: users
    \item $r_{a, p}$: user $a$'s rating for item $p$
    \item $\bar{r_a}$: user $a$'s average rating
    \item $P_a, P_b$: the sets of items users $a$ and $b$ has rated
    \item $P = P_a \cap P_b$: the items both users $a$ and $b$ has rated
\end{itemize}
This makes two like minded users, one being positive, one being negative, agree more.

A common prediction function is:
\[
    \mathrm{pred}(a, p) = \bar{r_a} + \sum_{b \in N}w(a, b)(r_{b, p} - \bar{r_b})
\]
with the weight function being the normalized similarity:
\[
    w(a, b) = \frac{\mathrm{sim}(a, b)}{\sum_{b \in N}\mathrm{sim}(a, b)}
\]
A common approach is to use between 50 and 200 positively correlated neighbors.

\subsubsection{Item-based $k$NN}
This approach uses similarities between items, instead of users, by finding items similar to some item to predict a users rating for it.

The similarity measure could be the Adjusted Cosine Similarity, which takes the average user rating into account.
\[
    \mathrm{sim}(p, q) = \frac{\sum_{u \in U}(r_{u, p} - \bar{r_u})(r_{u, q} - \bar{r_u})}{\sqrt{\sum_{u \in U}(r_{u, p} - \bar{r_u})^2}\sqrt{\sum_{u \in U}(r_{u, q} - \bar{r_u})^2}}
\]
where $U$ is the set of users who have rated both items $p$ and $q$, and $I$ is the set of items rated by user $a$, and the prediction function is:
\[
    \mathrm{pred}(a, p) = \sum_{q \in I}w(p, q)r_{a, q}
\]
and the weight function is:
\[
    w(p, q) = \frac{\mathrm{sim}(p, q)}{\sum_{q \in N}\mathrm{sim}(p, q)}
\]

\subsection{Matrix factorization}


\subsubsection{Is Matrix Factorization a better choice for CF for the Netflix data?}


\subsection{Pre-processing}
To save processing time, we subtract the user and movie means, ignoring missing entries:
\[
    R_{mu} \gets R_{mu} - \frac{1}{U_m}\sum_{s}{R_{ms}} - \frac{1}{M_u}\sum_{r}{R_{ru}} + \frac{1}{N}\sum_{s}\sum_{r}{R_{sr}}
\]
where
\begin{itemize}
    \item $U_m$: total number of ratings for movie $m$.
    \item $M_u$: total number of ratings for user $u$.
    \item $N$: total number of movie-user pairs.
\end{itemize}
%
When doing the predictions, this must be added to the result (replacing + with -, and - with +).

When doing the pre-processing on a limited set (100 movies, with around 100 ratings each), the mean results to very close to 0. We blame the difference on the double datatype.